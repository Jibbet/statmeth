{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, uniform # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html\n",
    "from scipy.optimize import minimize # https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
    "\n",
    "import matplotlib as mpl\n",
    "# to make plots bigger in Notebook\n",
    "mpl.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.5\n",
    "_______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sample sizes, mu and sigma's\n",
    "# for testing your algorithm, use a smaller number of samples first, e.g. 100!\n",
    "\n",
    "N=1000 #samples\n",
    "n=250 #sample size\n",
    "\n",
    "\n",
    "p=0.7\n",
    "s1=1\n",
    "s2=np.sqrt(10)\n",
    "mu=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate, fit and plot for one sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function which generates a sample:\n",
    "\n",
    "# You can use a Monte Carlo Trick for the generation of a sample from a mixed distribution which you have already seen in last week's exercises.\n",
    "# for each value within the sample:\n",
    "#   with probability p:\n",
    "#     draw a value from the distribution with sigma 1 use norm.rvs(mu,sigma,size=1)\n",
    "#   else: \n",
    "#     draw a value from the distribution with sigma 2 use norm.rvs(mu,sigma,size=1)\n",
    "\n",
    "def gen_sample(n,p,mu,sigma1,sigma2):\n",
    "    # TODO\n",
    "    \n",
    "    return sample\n",
    "\n",
    "test_sample = gen_sample(n,p,mu,s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function for the PDF - you can use norm.pdf(<x_value>, loc=<mu>, scale=<sigma>) which is a numpy function and can take a vector as an input!:\n",
    "def pdf(x,p,mu,sigma1,sigma2):\n",
    "    \n",
    "    return #TODO\n",
    "\n",
    "# write a function that generates the negative log-likelihood from the pdf. Important: write it in such a way, that it works nicely with the scipy.optimize.minimize function!\n",
    "# Note that s1 and s2 are fixed and not part of the parameters to be minimized in this example\n",
    "\n",
    "def nll (par, sample):\n",
    "    mu, p = par[0], par[1]\n",
    "    return #TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the negative log-likelihood function (as a function of p and mu) for one sample - this is a 3D plot. \n",
    "# You can use a visualisation method of your choice i.e. surface plot , contour plot or something similar.\n",
    "\n",
    "# For many of these methods you need a mesh - we generated one for you:\n",
    "\n",
    "x = np.linspace(-1, 1, 30)\n",
    "y = np.linspace(0.5, 0.9, 30)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.zeros_like(X)\n",
    "for i,x_ in enumerate(X[0]):\n",
    "    for j,y_ in enumerate(Y[:,0]):\n",
    "        Z[i,j] = nll([x_,y_], test_sample)\n",
    "\n",
    "#TODO visualize the negative likelihood function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now minimze the nll for your sample and print the results\n",
    "# you can use minimize (<function_to_minimize>, x0 = [ 0., .7 ], args = <sample>, bounds = ( (-.4,.4),(.5,.9) ) )\n",
    "# have a look into the documentation of the scipy minimize and try out some different minimzer methods \n",
    "# - what kind of output are the giving (Hesse Matrix yes or no?), where do you need starting values, where bounds?\n",
    "# which method is most suited for this exercise? \n",
    "    \n",
    "res = minimize( nll, #TODO )\n",
    "\n",
    "print(res)\n",
    "\n",
    "# print mu and p:\n",
    "print(\"mu = \",mu_hat := res.x[0])\n",
    "print(\"p = \",p_hat := res.x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of the sample together with the pdf, once with your ML estimators and once with the true values\n",
    "# the easiest way to get the normalisation right is by using the density=True argument in the plt.hist() function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start a loop over the number of samples\n",
    "\n",
    "mus,ps=[],[]\n",
    "for N_ in range(N):\n",
    "    # generate the sample - no need to save this after each step!\n",
    "    sample = gen_sample(n,p,mu,s1,s2)\n",
    "\n",
    "    # do the mimization\n",
    "    res = minimize ( #TODO )\n",
    "\n",
    "    mus.append ( res.x[0] )\n",
    "    ps.append ( res.x[1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and print mean values and standard deviations of the estimators\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histograms for the mus and ps\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.6\n",
    "__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with fixing all parameters, later you can reuse the function to generate samples from above, or just reuse the samples, if you saved them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sample sizes, mu and sigma's\n",
    "# for testing your algorithm, use a smaller number of samples first\n",
    "N=150 #samples\n",
    "n=250 #sample size\n",
    "\n",
    "\n",
    "p=0.7\n",
    "s1=1\n",
    "s2=np.sqrt(10)\n",
    "mu=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that calculates the EM algorithm\n",
    "\n",
    "def em_algorithm(x0, args):\n",
    "    \n",
    "    # this function follows the steps from lecture slide 327:\n",
    "    # https://github.com/WolfgangWaltenberger/classroom/blob/master/stat2020/Folien.pdf\n",
    "    \n",
    "    # 1) get the initial values\n",
    "    mu, p = x0[0], x0[1]\n",
    "    sample = args\n",
    "    \n",
    "    # this is an iterative algorithm until convergance, set some error threshold for stop\n",
    "    err_mu = 0.001\n",
    "    err_p = 0.001\n",
    "    \n",
    "    # if we do not reach convergence, stop after number of steps\n",
    "    max_steps = 50\n",
    "    \n",
    "    # init some variable to count the iteration steps\n",
    "    counter = 0\n",
    "    \n",
    "    while True:\n",
    "        # 2) calculate pij and pj\n",
    "        # Hint: i is the index of the sample value, j is the index of the distribution component\n",
    "        pij = np.empty([n, 2], dtype=float) # 2 distribution components, n sample values\n",
    "        pj = np.empty([2], dtype=float) # 2 distribution components\n",
    "\n",
    "        # TODO: fill the pij and pj values, you can either do this with a for-loop (slow) or directly with the help of numpy arrays.\n",
    "\n",
    "        # 3) estimate mu and p\n",
    "        p_new = #TODO # this is wj for j = 0\n",
    "        mu_new = #TODO # this is muj for j = 0\n",
    "\n",
    "        # 4) these steps are repeated until convergance\n",
    "        \n",
    "        if np.abs(mu_new - mu) < err_mu and np.abs(p_new - p) < err_p:\n",
    "            mu = np.copy(mu_new)\n",
    "            p = np.copy(p_new)\n",
    "            counter += 1\n",
    "            print('Convergence after {} steps.'.format(counter))\n",
    "            break\n",
    "        else:\n",
    "            mu = np.copy(mu_new)\n",
    "            p = np.copy(p_new)\n",
    "            counter += 1\n",
    "        if counter == max_steps:\n",
    "            print('No convergece after {} steps, iteration stopped.'.format(counter))\n",
    "            break\n",
    "    \n",
    "    return mu, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start a loop over the samples.\n",
    "# Depending on your N and the implementation of the EM algorithm this can take up to several minutes! \n",
    "# If it takes too long, feel free to adapt the parameter N.\n",
    "\n",
    "mus,ps=[],[]\n",
    "for N_ in range(N):\n",
    "    # generate the sample - no need to save this after each step!\n",
    "    sample = gen_sample(n,p,mu,s1,s2)\n",
    "\n",
    "    # use the EM algorithm\n",
    "    x0 = # TODO: think about your starting values, you can also choose random values as initial guesses\n",
    "    mu, p = em_algorithm(x0 = x0, args = sample) \n",
    "\n",
    "    mus.append(mu)\n",
    "    ps.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean values and standard deviations of the estimators\n",
    "# the nanmean() and nanstd() exclude nan values, that might occure during the optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histograms for the mus and ps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "9beab74f5023144e28ef7b1475c226b38d7545972a79393e3fc113252fce1d16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
