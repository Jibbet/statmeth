\section{Sheet}
\subsection{Bernoulli}
\subsubsection{Clopper and Pearson confidence interval}
A Bernoulli experiment is repeated $n=200$ times with $k=121$ sucesses. Calculate the symmetric $95\%$ interval
for the  parameter $p$.
The Interval boundaries can be calculated with the inverse beta distribution.
\begin{align}
    G_1\left(k\right)&=\beta\left(\frac{\alpha}{2};k,n-k+1\right)\approx0.534
    \\G_2\left(k\right)&=\beta\left(\frac{1-\alpha}{2};k+1,n-k\right)\approx0.673
\end{align}
\subsubsection{Approximation by normal distribution}
\paragraph{bootstrap}
\begin{align}
    G_1\left(k\right)&=\hat{p}-z_{1-\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}\approx0.537
    \\G_2\left(k\right)&=\hat{p}+z_{1-\alpha/2}\sqrt{\frac{\hat{p}\left(1-\hat{p}\right)}{n}}\approx0.673
\end{align}
\paragraph{robust}
\begin{align}
    G_1\left(k\right)&=\hat{p}-z_{1-\alpha/2}\frac{1}{2\sqrt{n}}\approx0.536
    \\G_2\left(k\right)&=\hat{p}+z_{1-\alpha/2}\frac{1}{2\sqrt{n}}\approx0.674
\end{align}
\subsubsection{Agresti-Coull}
Choose $k'=k+\left(z_{1-\alpha/2}\right)^2$, $n'=n+\left(z_{1-\alpha/2}\right)^2$ and $p'=k'/n'$.
\begin{align}
    G_1\left(k\right)&=p'-z_{1-\alpha/2}\sqrt{\frac{p'\left(1-p'\right)}{n}}\approx0.546
    \\G_2\left(k\right)&=p'+z_{1-\alpha/2}\sqrt{\frac{p'\left(1-p'\right)}{n}}\approx0.679
\end{align}
\subsection{Biased and unbiased Estimators for uniform distribution interval borders}
The joint probability of the sample ist
\begin{equation}
    g\left(X_1,\dots,X_N\vert a,b\right)=\prod_n^N \frac{1}{b-a}\cdot I_{\left[a,b\right]}\left(X_n\right)
\end{equation}
The ML estimator is
\begin{equation}
    \hat{a}=\text{arg max}_a\prod_{n}^{N}\frac{1}{b-a}\cdot I_{\left[a,b\right]}\left(X_n\right)
\end{equation}
This would not take a minimum if not for the constraint
\begin{equation}
    \hat{a}\leq\min_n\left\{X_n\right\}
\end{equation}
Therefore
\begin{equation}
    \hat{a}=\min_n\left\{X_n\right\}
\end{equation}
Similarly
\begin{equation}
    \hat{b}\geq\max_n\left\{X_n\right\}
\end{equation}
and
\begin{equation}
    \hat{b}=\max_n\left\{X_n\right\}
\end{equation}
\paragraph{Showing the estimators are biased}
To show the estimators are biased, calculate their distribution functions:
\begin{align}
    F_{\hat{a}}\left(x\right)=P\left(\hat{a}\leq x\right)&=1-\prod_n P\left(X_n>x\right)
    \\&=1-P^N\left(X>x\right)
    \\&=1-\left(\frac{b-x}{b-a}\right)^N
\end{align}
The density is
\begin{align}
    \pdv{F_{\hat{a}}}{x}&=\frac{N}{b-a}\left(\frac{b-x}{b-a}\right)^{N-1}
\end{align}
Now the expectation value is
\begin{align}
    E\left(\hat{a}\right)&=\int_a^b x \frac{n}{b-a}\left(\frac{b-x}{b-a}\right)^{N-1}\;dx
    \\&=\left[-\left(\frac{b-x}{b-a}\right)^N x\right]_a^b+\int_a^b\left(\frac{b-x}{b-a}\right)^N\;dx
\end{align}
Here is
\begin{align}
    \left[-\left(\frac{b-x}{b-a}\right)^N x\right]_a^b=\left[-\underbrace{\left(\frac{b-b}{b-a}\right)^N}_{=0}b + \underbrace{\left(\frac{b-a}{b-a}\right)^N}_{=1}a\right]=a
\end{align}
and
\begin{align}
    \int_a^b\left(\frac{b-x}{b-a}\right)^N\;dx&=\left[-\frac{b-a}{N+1}\left(\frac{b-x}{b-a}\right)^{N+1}\right]_a^b
    \\&=-\frac{b-a}{N+1}\left[\underbrace{\left(\frac{b-b}{b-a}\right)^{N+1}}_{=0}-\underbrace{\left(\frac{b-a}{b-a}\right)^{N+1}}_{=1}\right]
    \\&=\frac{b-a}{N+1}
\end{align}
Together
\begin{align}
    E\left(\hat{a}\right)=a+\frac{b-a}{N+1}
\end{align}
Similarly for $\hat{b}$:
\begin{align}
    F_{\hat{b}}\left(x\right)&=P\left(\hat{b}\leq x\right)
    \\&=\prod_n P\left(X_n\leq x\right)
    \\&=P^N\left(X\leq x\right)
    \\&=\left(\frac{x-a}{b-a}\right)^N
\end{align}
The density is
\begin{equation}
    \pdv{F_{\hat{b}}}{x}=\frac{N}{b-a}\left(\frac{x-a}{b-a}\right)^{N-1}
\end{equation}
The expectation value of $\hat{b}$ is
\begin{align}
    E(\hat{b})&=\int_{a}^{b}x\frac{N}{b-a}\left(\frac{x-a}{b-a}\right)^{N-1}\;dx
    \\&=\left[\left(\frac{x-a}{b-a}\right)^N x\right]_a^b-\int_{a}^{b}\left(\frac{x-a}{b-a}\right)^N\;dx
\end{align}
where
\begin{align}
    \left[\left(\frac{x-a}{b-a}\right)^N x\right]_a^b&=\left[\underbrace{\left(\frac{b-a}{b-a}\right)^N}_{=1}b-\underbrace{\left(\frac{a-a}{b-a}\right)^N}_{=0}a\right]=b
\end{align}
and
\begin{align}
    \int_{a}^{b}\left(\frac{x-a}{b-a}\right)^N\;dx&=\left[\frac{b-a}{N+1}\left(\frac{x-a}{b-a}\right)^{N+1}\right]_a^b
    \\&=\frac{b-a}{N+1}\left[\underbrace{\left(\frac{b-a}{b-a}\right)^{N+1}}_{=1}-\underbrace{\left(\frac{a-a}{b-a}\right)^{N+1}}_{=0}\right]
    \\&=\frac{b-a}{N+1}
\end{align}
Together
\begin{align}
    E(\hat{b})=b-\frac{b-a}{N+1}
\end{align}
\paragraph{Conclusion} Both estimators are biased, but asymptotically unbiased. This makes intuitivly sense.
We would like to correct the estimators $\hat{a}$ and $\hat{b}$.
\begin{align}
    \hat{a}_c &= \hat{a}-\frac{b-a}{N+1}
    \\\hat{b}_c &= \hat{b}+\frac{b-a}{N+1}
\end{align}
but $a$ and $b$ are not a priori known.
Lets just guess an estimator.
\begin{align}
    E(\hat{b}-\hat{a})&=E(\hat{b})-E(\hat{a})
    \\&=b-a-\frac{2\left(b-a\right)}{N+1}
    \\&=\frac{\left(b-a\right)\left(N+1\right)}{N+1}-\frac{2\left(b-a\right)}{N+1}
    \\&=\left(b-a\right)\frac{N-1}{N+1}
    \\\Leftrightarrow E\left(\frac{N+1}{N-1}\left(\hat{b}-\hat{a}\right)\right)&=b-a
\end{align}
Not quite, but this means, that
\begin{align}
    \frac{N+1}{N-1}\left(\hat{b}-\hat{a}\right)
\end{align}
is unbiased estimator for $b-a$. Inserting the newly found estimator yields
\begin{align}
    \hat{a}_c &= \hat{a}-\frac{N+1}{N-1}\frac{\hat{b}-\hat{a}}{N+1}
    \\&=\frac{\hat{a}\left(N-1\right)}{N-1}-\frac{\hat{b}-\hat{a}}{N-1}
    \\&=\frac{\hat{a}N-\hat{b}}{N-1}
\end{align}
\begin{align}
    \hat{b}_c&=\hat{b}+\frac{N+1}{N-1}\frac{\hat{b}-\hat{a}}{N+1}
    \\&=\frac{\hat{b}\left(N-1\right)}{N-1}+\frac{\hat{b}-\hat{a}}{N-1}
    \\&=\frac{\hat{b}N-\hat{a}}{N-1}
\end{align}
and we are done.